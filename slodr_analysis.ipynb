{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman's Law of Diminishing Returns (SLODR) Analysis for Epoch Capability Index\n",
    "\n",
    "This notebook tests whether **Spearman's law of diminishing returns** holds for AI models using the **Epoch Capability Index (ECI)**.\n",
    "\n",
    "## Hypothesis\n",
    "At higher ECI levels, the first principal component's explained variance ratio (EVR1) should **decrease**, indicating that general capability becomes less dominant and specific abilities become more differentiated.\n",
    "\n",
    "## Methodology\n",
    "1. Load benchmark data and ECI scores\n",
    "2. Preprocess: logit transformation, KNN imputation, standardization\n",
    "3. Split models into ECI bins\n",
    "4. Perform PCA within each bin\n",
    "5. Analyze EVR1 vs ECI relationship\n",
    "6. Statistical testing: bootstrap confidence intervals and permutation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "from scipy.special import logit, expit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_data(bench_dir='benchmark_data', score_col='mean_score', model_col='Model version'):\n",
    "    \"\"\"\n",
    "    Load all benchmark CSV files from directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bench_dir : str\n",
    "        Directory containing benchmark CSV files\n",
    "    score_col : str\n",
    "        Column name for scores (default: 'mean_score')\n",
    "    model_col : str\n",
    "        Column name for model identifiers (default: 'Model version')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : {benchmark_name: DataFrame with 'model' and 'score' columns}\n",
    "    \"\"\"\n",
    "    bench_path = Path(bench_dir)\n",
    "    benchmarks = {}\n",
    "    \n",
    "    # Get all CSV files except the ECI file\n",
    "    csv_files = [f for f in bench_path.glob('*.csv') if 'epoch_capabilities_index' not in f.name]\n",
    "    \n",
    "    print(f\"Loading {len(csv_files)} benchmark files...\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        benchmark_name = csv_file.stem  # filename without extension\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if model_col not in df.columns:\n",
    "                print(f\"  ⚠ Skipping {benchmark_name}: missing '{model_col}' column\")\n",
    "                continue\n",
    "                \n",
    "            # Try to find score column\n",
    "            if score_col in df.columns:\n",
    "                score_column = score_col\n",
    "            elif 'Best score (across scorers)' in df.columns:\n",
    "                score_column = 'Best score (across scorers)'\n",
    "            elif 'score' in df.columns:\n",
    "                score_column = 'score'\n",
    "            else:\n",
    "                print(f\"  ⚠ Skipping {benchmark_name}: no score column found\")\n",
    "                continue\n",
    "            \n",
    "            # Extract model and score columns\n",
    "            df_clean = df[[model_col, score_column]].copy()\n",
    "            df_clean.columns = ['model', 'score']\n",
    "            \n",
    "            # Remove rows with missing scores\n",
    "            df_clean = df_clean.dropna(subset=['score'])\n",
    "            \n",
    "            # Convert score to numeric\n",
    "            df_clean['score'] = pd.to_numeric(df_clean['score'], errors='coerce')\n",
    "            df_clean = df_clean.dropna(subset=['score'])\n",
    "            \n",
    "            # For models that appear multiple times, take the max score\n",
    "            df_clean = df_clean.groupby('model')['score'].max().reset_index()\n",
    "            \n",
    "            if len(df_clean) > 0:\n",
    "                benchmarks[benchmark_name] = df_clean\n",
    "                print(f\"  ✓ Loaded {benchmark_name}: {len(df_clean)} models\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Skipping {benchmark_name}: no valid data\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {benchmark_name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n✓ Successfully loaded {len(benchmarks)} benchmarks\\n\")\n",
    "    return benchmarks\n",
    "\n",
    "# Load benchmarks\n",
    "benchmarks = load_benchmark_data()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nBenchmark Summary:\")\n",
    "for name, df in list(benchmarks.items())[:10]:\n",
    "    print(f\"  {name}: {len(df)} models, score range [{df['score'].min():.3f}, {df['score'].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load ECI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eci_data(eci_csv='benchmark_data/epoch_capabilities_index.csv', \n",
    "                  model_col='Model version', \n",
    "                  eci_col='ECI Score'):\n",
    "    \"\"\"\n",
    "    Load the Epoch Capability Index (ECI) data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eci_csv : str\n",
    "        Path to the ECI CSV file\n",
    "    model_col : str\n",
    "        Column name for model identifiers\n",
    "    eci_col : str\n",
    "        Column name for ECI scores\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with 'model' and 'eci' columns\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(eci_csv)\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    df_eci = df[[model_col, eci_col]].copy()\n",
    "    df_eci.columns = ['model', 'eci']\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_eci = df_eci.dropna(subset=['eci'])\n",
    "    \n",
    "    # Convert ECI to numeric\n",
    "    df_eci['eci'] = pd.to_numeric(df_eci['eci'], errors='coerce')\n",
    "    df_eci = df_eci.dropna(subset=['eci'])\n",
    "    \n",
    "    print(f\"✓ Loaded ECI data: {len(df_eci)} models\")\n",
    "    print(f\"  ECI range: [{df_eci['eci'].min():.2f}, {df_eci['eci'].max():.2f}]\")\n",
    "    print(f\"  ECI mean: {df_eci['eci'].mean():.2f}, median: {df_eci['eci'].median():.2f}\")\n",
    "    \n",
    "    return df_eci\n",
    "\n",
    "# Load ECI data\n",
    "eci_data = load_eci_data()\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample ECI data:\")\n",
    "display(eci_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Combined Dataset\n",
    "\n",
    "Merge benchmarks into a wide matrix: rows = models, columns = benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wide_matrix(benchmarks, eci_data):\n",
    "    \"\"\"\n",
    "    Create a wide matrix with models as rows and benchmarks as columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    benchmarks : dict\n",
    "        Dictionary of benchmark DataFrames\n",
    "    eci_data : DataFrame\n",
    "        ECI data with 'model' and 'eci' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with models as index, benchmarks as columns, and ECI column\n",
    "    \"\"\"\n",
    "    # Start with ECI data\n",
    "    df_wide = eci_data.copy()\n",
    "    df_wide = df_wide.set_index('model')\n",
    "    \n",
    "    # Add each benchmark as a column\n",
    "    for bench_name, bench_df in benchmarks.items():\n",
    "        bench_df_indexed = bench_df.set_index('model')\n",
    "        df_wide[bench_name] = bench_df_indexed['score']\n",
    "    \n",
    "    # Reset index to make model a column\n",
    "    df_wide = df_wide.reset_index()\n",
    "    \n",
    "    print(f\"✓ Created wide matrix: {len(df_wide)} models × {len(benchmarks)} benchmarks\")\n",
    "    print(f\"  Total data points: {len(df_wide) * len(benchmarks)}\")\n",
    "    \n",
    "    # Calculate coverage statistics\n",
    "    benchmark_cols = [col for col in df_wide.columns if col not in ['model', 'eci']]\n",
    "    coverage = df_wide[benchmark_cols].notna().sum() / len(df_wide)\n",
    "    print(f\"  Mean benchmark coverage: {coverage.mean():.1%}\")\n",
    "    \n",
    "    # Show missingness per model\n",
    "    missing_per_model = df_wide[benchmark_cols].isna().sum(axis=1)\n",
    "    print(f\"  Models with <50% coverage: {(missing_per_model > len(benchmark_cols)/2).sum()}\")\n",
    "    \n",
    "    return df_wide\n",
    "\n",
    "# Create wide matrix\n",
    "df_wide = create_wide_matrix(benchmarks, eci_data)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data matrix:\")\n",
    "display(df_wide.head())\n",
    "\n",
    "print(\"\\nColumns:\")\n",
    "print(df_wide.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Functions\n",
    "\n",
    "### 4.1 Logit Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_logit_transform(scores, eps=1e-4):\n",
    "    \"\"\"\n",
    "    Apply logit transformation to handle ceiling effects.\n",
    "    logit(p) = log(p / (1-p))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scores : array-like\n",
    "        Scores in [0, 1] range\n",
    "    eps : float\n",
    "        Small value to avoid log(0) errors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Logit-transformed scores\n",
    "    \"\"\"\n",
    "    scores_clipped = np.clip(scores, eps, 1 - eps)\n",
    "    return np.log(scores_clipped / (1 - scores_clipped))\n",
    "\n",
    "def apply_expit_transform(logit_scores):\n",
    "    \"\"\"\n",
    "    Apply inverse logit (sigmoid) transformation.\n",
    "    expit(x) = 1 / (1 + exp(-x))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-logit_scores))\n",
    "\n",
    "print(\"✓ Transformation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_knn(X, k=5):\n",
    "    \"\"\"\n",
    "    Impute missing values using k-Nearest Neighbors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Data matrix with missing values\n",
    "    k : int\n",
    "        Number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Imputed data matrix\n",
    "    \"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Count imputed values\n",
    "    n_missing = np.isnan(X).sum()\n",
    "    print(f\"  Imputed {n_missing} missing values using KNN (k={k})\")\n",
    "    \n",
    "    return X_imputed\n",
    "\n",
    "print(\"✓ KNN imputation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Z-score standardization: (X - mean) / std\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Data matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Standardized data, mean, std\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0, ddof=1)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    std[std == 0] = 1.0\n",
    "    \n",
    "    X_standardized = (X - mean) / std\n",
    "    \n",
    "    return X_standardized, mean, std\n",
    "\n",
    "print(\"✓ Standardization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_wide, use_logit=True, k=5):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract benchmark scores\n",
    "    2. Apply logit transformation (optional)\n",
    "    3. KNN imputation\n",
    "    4. Inverse logit transformation (if logit was applied)\n",
    "    5. Standardization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_wide : DataFrame\n",
    "        Wide matrix with models, ECI, and benchmark scores\n",
    "    use_logit : bool\n",
    "        Whether to apply logit transformation\n",
    "    k : int\n",
    "        Number of neighbors for KNN imputation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_preprocessed, model_names, benchmark_names, eci_values\n",
    "    \"\"\"\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    \n",
    "    # Extract components\n",
    "    model_names = df_wide['model'].values\n",
    "    eci_values = df_wide['eci'].values\n",
    "    benchmark_cols = [col for col in df_wide.columns if col not in ['model', 'eci']]\n",
    "    benchmark_names = benchmark_cols\n",
    "    \n",
    "    # Extract benchmark scores\n",
    "    X = df_wide[benchmark_cols].values\n",
    "    \n",
    "    print(f\"  Original shape: {X.shape}\")\n",
    "    print(f\"  Missing values: {np.isnan(X).sum()} ({np.isnan(X).mean():.1%})\")\n",
    "    \n",
    "    # Step 1: Logit transformation (optional)\n",
    "    if use_logit:\n",
    "        print(\"  Applying logit transformation...\")\n",
    "        X_logit = np.full_like(X, np.nan)\n",
    "        mask = ~np.isnan(X)\n",
    "        X_logit[mask] = apply_logit_transform(X[mask])\n",
    "        X = X_logit\n",
    "    \n",
    "    # Step 2: KNN imputation\n",
    "    print(\"  Performing KNN imputation...\")\n",
    "    X_imputed = impute_missing_knn(X, k=k)\n",
    "    \n",
    "    # Step 3: Inverse logit (if logit was applied)\n",
    "    if use_logit:\n",
    "        print(\"  Applying inverse logit transformation...\")\n",
    "        X_imputed = apply_expit_transform(X_imputed)\n",
    "    \n",
    "    # Step 4: Standardization\n",
    "    print(\"  Standardizing...\")\n",
    "    X_standardized, _, _ = standardize(X_imputed)\n",
    "    \n",
    "    print(f\"  Final shape: {X_standardized.shape}\")\n",
    "    print(f\"  Mean: {X_standardized.mean():.6f}, Std: {X_standardized.std():.6f}\")\n",
    "    print(\"\\n✓ Preprocessing complete\\n\")\n",
    "    \n",
    "    return X_standardized, model_names, benchmark_names, eci_values\n",
    "\n",
    "# Preprocess data\n",
    "X_preprocessed, model_names, benchmark_names, eci_values = preprocess_data(df_wide, use_logit=True, k=5)\n",
    "\n",
    "print(f\"Preprocessed data: {X_preprocessed.shape[0]} models × {X_preprocessed.shape[1]} benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: PCA per ECI Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_per_bin(X, eci_values, n_bins=8):\n",
    "    \"\"\"\n",
    "    Perform PCA within each ECI bin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Preprocessed data matrix\n",
    "    eci_values : array-like\n",
    "        ECI scores for each model\n",
    "    n_bins : int\n",
    "        Number of equal-count bins\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with bin statistics: mean_eci, evr1, n_models\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming PCA per ECI bin ({n_bins} bins)...\\n\")\n",
    "    \n",
    "    # Sort by ECI\n",
    "    sort_idx = np.argsort(eci_values)\n",
    "    X_sorted = X[sort_idx]\n",
    "    eci_sorted = eci_values[sort_idx]\n",
    "    \n",
    "    # Create equal-count bins\n",
    "    bin_edges = np.percentile(eci_sorted, np.linspace(0, 100, n_bins + 1))\n",
    "    bin_indices = np.digitize(eci_sorted, bin_edges[1:-1])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for bin_idx in range(n_bins):\n",
    "        # Get models in this bin\n",
    "        mask = (bin_indices == bin_idx)\n",
    "        X_bin = X_sorted[mask]\n",
    "        eci_bin = eci_sorted[mask]\n",
    "        \n",
    "        if len(X_bin) < 2:\n",
    "            print(f\"  Bin {bin_idx+1}: Skipped (only {len(X_bin)} models)\")\n",
    "            continue\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA()\n",
    "        pca.fit(X_bin)\n",
    "        \n",
    "        # Extract first PC explained variance\n",
    "        evr1 = pca.explained_variance_ratio_[0]\n",
    "        mean_eci = np.mean(eci_bin)\n",
    "        \n",
    "        results.append({\n",
    "            'bin': bin_idx + 1,\n",
    "            'mean_eci': mean_eci,\n",
    "            'min_eci': np.min(eci_bin),\n",
    "            'max_eci': np.max(eci_bin),\n",
    "            'evr1': evr1,\n",
    "            'evr2': pca.explained_variance_ratio_[1] if len(pca.explained_variance_ratio_) > 1 else np.nan,\n",
    "            'n_models': len(X_bin)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Bin {bin_idx+1}: ECI [{mean_eci:.1f}], EVR1 = {evr1:.3f}, n = {len(X_bin)}\")\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ PCA complete for {len(df_results)} bins\\n\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Perform PCA per bin\n",
    "pca_results = perform_pca_per_bin(X_preprocessed, eci_values, n_bins=8)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPCA Results:\")\n",
    "display(pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_slodr_hypothesis(pca_results):\n",
    "    \"\"\"\n",
    "    Test SLODR hypothesis using linear regression.\n",
    "    \n",
    "    Expected: Negative slope (EVR1 decreases with higher ECI)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_results : DataFrame\n",
    "        PCA results with mean_eci and evr1 columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with slope, intercept, r2, mse\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing SLODR hypothesis...\\n\")\n",
    "    \n",
    "    X = pca_results['mean_eci'].values.reshape(-1, 1)\n",
    "    y = pca_results['evr1'].values\n",
    "    \n",
    "    # Fit linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Metrics\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    # Pearson correlation\n",
    "    correlation, p_value = stats.pearsonr(pca_results['mean_eci'], pca_results['evr1'])\n",
    "    \n",
    "    results = {\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'correlation': correlation,\n",
    "        'correlation_p_value': p_value,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"  Slope: {slope:.6f}\")\n",
    "    print(f\"  Intercept: {intercept:.6f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  Correlation: {correlation:.4f} (p = {p_value:.4f})\")\n",
    "    \n",
    "    if slope < 0:\n",
    "        print(\"\\n  ✓ SLODR hypothesis supported (negative slope)\")\n",
    "    else:\n",
    "        print(\"\\n  ✗ SLODR hypothesis NOT supported (positive slope)\")\n",
    "    \n",
    "    print(\"\\n✓ Regression analysis complete\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze SLODR hypothesis\n",
    "regression_results = analyze_slodr_hypothesis(pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(pca_results, n_iterations=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for the slope.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_results : DataFrame\n",
    "        PCA results\n",
    "    n_iterations : int\n",
    "        Number of bootstrap samples\n",
    "    confidence : float\n",
    "        Confidence level (e.g., 0.95 for 95%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with lower_bound, upper_bound, slopes\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming bootstrap analysis ({n_iterations} iterations)...\\n\")\n",
    "    \n",
    "    slopes = []\n",
    "    n_samples = len(pca_results)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Resample with replacement\n",
    "        sample_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        sample_data = pca_results.iloc[sample_idx]\n",
    "        \n",
    "        # Fit regression\n",
    "        X = sample_data['mean_eci'].values.reshape(-1, 1)\n",
    "        y = sample_data['evr1'].values\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        slopes.append(model.coef_[0])\n",
    "    \n",
    "    slopes = np.array(slopes)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    lower_bound = np.percentile(slopes, lower_percentile)\n",
    "    upper_bound = np.percentile(slopes, upper_percentile)\n",
    "    \n",
    "    print(f\"  Bootstrap slope distribution:\")\n",
    "    print(f\"    Mean: {slopes.mean():.6f}\")\n",
    "    print(f\"    Std: {slopes.std():.6f}\")\n",
    "    print(f\"    {confidence*100:.0f}% CI: [{lower_bound:.6f}, {upper_bound:.6f}]\")\n",
    "    \n",
    "    if lower_bound < 0 and upper_bound < 0:\n",
    "        print(f\"\\n  ✓ Confidence interval excludes zero (significant negative slope)\")\n",
    "    elif lower_bound > 0 and upper_bound > 0:\n",
    "        print(f\"\\n  ! Confidence interval excludes zero (significant positive slope)\")\n",
    "    else:\n",
    "        print(f\"\\n  ✗ Confidence interval includes zero (not significant)\")\n",
    "    \n",
    "    print(\"\\n✓ Bootstrap complete\\n\")\n",
    "    \n",
    "    return {\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'slopes': slopes,\n",
    "        'mean': slopes.mean(),\n",
    "        'std': slopes.std()\n",
    "    }\n",
    "\n",
    "# Bootstrap analysis\n",
    "bootstrap_results = bootstrap_confidence_interval(pca_results, n_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(pca_results, observed_slope, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform permutation test to assess significance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_results : DataFrame\n",
    "        PCA results\n",
    "    observed_slope : float\n",
    "        The observed slope from the original data\n",
    "    n_permutations : int\n",
    "        Number of permutations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with p_value, permuted_slopes\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming permutation test ({n_permutations} permutations)...\\n\")\n",
    "    \n",
    "    permuted_slopes = []\n",
    "    X = pca_results['mean_eci'].values.reshape(-1, 1)\n",
    "    y_original = pca_results['evr1'].values\n",
    "    \n",
    "    for i in range(n_permutations):\n",
    "        # Permute EVR1 values\n",
    "        y_permuted = np.random.permutation(y_original)\n",
    "        \n",
    "        # Fit regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y_permuted)\n",
    "        permuted_slopes.append(model.coef_[0])\n",
    "    \n",
    "    permuted_slopes = np.array(permuted_slopes)\n",
    "    \n",
    "    # Calculate p-value (two-tailed)\n",
    "    # For negative observed slope, count how many permuted slopes are <= observed\n",
    "    if observed_slope < 0:\n",
    "        p_value = np.mean(permuted_slopes <= observed_slope) * 2\n",
    "    else:\n",
    "        p_value = np.mean(permuted_slopes >= observed_slope) * 2\n",
    "    \n",
    "    p_value = min(p_value, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    print(f\"  Observed slope: {observed_slope:.6f}\")\n",
    "    print(f\"  Permuted slopes mean: {permuted_slopes.mean():.6f}\")\n",
    "    print(f\"  Permuted slopes std: {permuted_slopes.std():.6f}\")\n",
    "    print(f\"  P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"\\n  ✓ Result is statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\n  ✗ Result is not statistically significant (p >= 0.05)\")\n",
    "    \n",
    "    print(\"\\n✓ Permutation test complete\\n\")\n",
    "    \n",
    "    return {\n",
    "        'p_value': p_value,\n",
    "        'permuted_slopes': permuted_slopes\n",
    "    }\n",
    "\n",
    "# Permutation test\n",
    "permutation_results = permutation_test(pca_results, regression_results['slope'], n_permutations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Creating visualizations...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 EVR1 vs Mean ECI with Regression Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_results['mean_eci'], pca_results['evr1'], \n",
    "           s=100, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "# Regression line\n",
    "X_plot = np.linspace(pca_results['mean_eci'].min(), pca_results['mean_eci'].max(), 100)\n",
    "y_plot = regression_results['model'].predict(X_plot.reshape(-1, 1))\n",
    "ax.plot(X_plot, y_plot, 'r-', linewidth=2, alpha=0.8, \n",
    "        label=f\"Linear fit: slope = {regression_results['slope']:.6f}\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Mean ECI', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('First PC Explained Variance Ratio (EVR1)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('SLODR Analysis: EVR1 vs ECI', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'evr1_vs_eci.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: output/evr1_vs_eci.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Permutation Test Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Histogram of permuted slopes\n",
    "ax.hist(permutation_results['permuted_slopes'], bins=50, alpha=0.7, \n",
    "        edgecolor='black', label='Permuted slopes')\n",
    "\n",
    "# Mark observed slope\n",
    "ax.axvline(regression_results['slope'], color='red', linewidth=2, \n",
    "           linestyle='--', label=f\"Observed slope = {regression_results['slope']:.6f}\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Slope', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f\"Permutation Test Distribution (p = {permutation_results['p_value']:.4f})\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'permutation_test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: output/permutation_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Bootstrap Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Histogram of bootstrap slopes\n",
    "ax.hist(bootstrap_results['slopes'], bins=50, alpha=0.7, \n",
    "        edgecolor='black', label='Bootstrap slopes')\n",
    "\n",
    "# Mark observed slope\n",
    "ax.axvline(regression_results['slope'], color='red', linewidth=2, \n",
    "           linestyle='--', label=f\"Observed slope = {regression_results['slope']:.6f}\")\n",
    "\n",
    "# Mark confidence interval\n",
    "ax.axvline(bootstrap_results['lower_bound'], color='green', linewidth=2, \n",
    "           linestyle=':', label=f\"95% CI: [{bootstrap_results['lower_bound']:.6f}, {bootstrap_results['upper_bound']:.6f}]\")\n",
    "ax.axvline(bootstrap_results['upper_bound'], color='green', linewidth=2, linestyle=':')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Slope', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Bootstrap Distribution of Slopes', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'bootstrap_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: output/bootstrap_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 ECI Distribution Across Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: EVR1 per bin\n",
    "ax1.bar(pca_results['bin'], pca_results['evr1'], alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('ECI Bin', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('EVR1', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('First PC Explained Variance by ECI Bin', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right plot: Sample size per bin\n",
    "ax2.bar(pca_results['bin'], pca_results['n_models'], alpha=0.7, \n",
    "        edgecolor='black', color='orange')\n",
    "ax2.set_xlabel('ECI Bin', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Models', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Sample Size by ECI Bin', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'bin_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: output/bin_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export wide matrix\n",
    "df_wide.to_csv(output_dir / 'wide_matrix.csv', index=False)\n",
    "print(\"✓ Saved: output/wide_matrix.csv\")\n",
    "\n",
    "# Export PCA results\n",
    "pca_results.to_csv(output_dir / 'pca_results.csv', index=False)\n",
    "print(\"✓ Saved: output/pca_results.csv\")\n",
    "\n",
    "# Export summary statistics\n",
    "summary = pd.DataFrame({\n",
    "    'metric': ['slope', 'intercept', 'r2', 'mse', 'correlation', 'correlation_p_value',\n",
    "               'bootstrap_lower', 'bootstrap_upper', 'bootstrap_mean', 'bootstrap_std',\n",
    "               'permutation_p_value'],\n",
    "    'value': [\n",
    "        regression_results['slope'],\n",
    "        regression_results['intercept'],\n",
    "        regression_results['r2'],\n",
    "        regression_results['mse'],\n",
    "        regression_results['correlation'],\n",
    "        regression_results['correlation_p_value'],\n",
    "        bootstrap_results['lower_bound'],\n",
    "        bootstrap_results['upper_bound'],\n",
    "        bootstrap_results['mean'],\n",
    "        bootstrap_results['std'],\n",
    "        permutation_results['p_value']\n",
    "    ]\n",
    "})\n",
    "summary.to_csv(output_dir / 'summary_statistics.csv', index=False)\n",
    "print(\"✓ Saved: output/summary_statistics.csv\")\n",
    "\n",
    "print(\"\\n✓ All results exported to output/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Robustness Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Analysis Without Logit Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS CHECK 1: Without Logit Transformation\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Preprocess without logit\n",
    "X_no_logit, _, _, _ = preprocess_data(df_wide, use_logit=False, k=5)\n",
    "\n",
    "# PCA per bin\n",
    "pca_results_no_logit = perform_pca_per_bin(X_no_logit, eci_values, n_bins=8)\n",
    "\n",
    "# Regression\n",
    "regression_no_logit = analyze_slodr_hypothesis(pca_results_no_logit)\n",
    "\n",
    "# Bootstrap\n",
    "bootstrap_no_logit = bootstrap_confidence_interval(pca_results_no_logit, n_iterations=1000)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  With logit:    slope = {regression_results['slope']:.6f}, p = {permutation_results['p_value']:.4f}\")\n",
    "print(f\"  Without logit: slope = {regression_no_logit['slope']:.6f}\")\n",
    "print(f\"  Difference:    {abs(regression_results['slope'] - regression_no_logit['slope']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Exclude Near-Ceiling Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS CHECK 2: Exclude Near-Ceiling Benchmarks\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def filter_ceiling_benchmarks(df_wide, threshold=0.98, top_pct=0.1):\n",
    "    \"\"\"\n",
    "    Remove benchmarks where top 10% of models have median score > threshold.\n",
    "    \"\"\"\n",
    "    benchmark_cols = [col for col in df_wide.columns if col not in ['model', 'eci']]\n",
    "    \n",
    "    # Sort by ECI and get top 10%\n",
    "    df_sorted = df_wide.sort_values('eci', ascending=False)\n",
    "    n_top = max(1, int(len(df_sorted) * top_pct))\n",
    "    df_top = df_sorted.head(n_top)\n",
    "    \n",
    "    # Check median score for top models in each benchmark\n",
    "    keep_benchmarks = []\n",
    "    for col in benchmark_cols:\n",
    "        median_score = df_top[col].median()\n",
    "        if pd.notna(median_score) and median_score <= threshold:\n",
    "            keep_benchmarks.append(col)\n",
    "        else:\n",
    "            print(f\"  Excluding {col}: top models median = {median_score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nKept {len(keep_benchmarks)}/{len(benchmark_cols)} benchmarks\")\n",
    "    \n",
    "    return df_wide[['model', 'eci'] + keep_benchmarks]\n",
    "\n",
    "# Filter benchmarks\n",
    "df_wide_filtered = filter_ceiling_benchmarks(df_wide, threshold=0.98, top_pct=0.1)\n",
    "\n",
    "if len(df_wide_filtered.columns) > 3:  # At least one benchmark remains\n",
    "    # Preprocess and analyze\n",
    "    X_filtered, _, _, _ = preprocess_data(df_wide_filtered, use_logit=True, k=5)\n",
    "    pca_results_filtered = perform_pca_per_bin(X_filtered, eci_values, n_bins=8)\n",
    "    regression_filtered = analyze_slodr_hypothesis(pca_results_filtered)\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  All benchmarks:     slope = {regression_results['slope']:.6f}\")\n",
    "    print(f\"  Filtered benchmarks: slope = {regression_filtered['slope']:.6f}\")\n",
    "    print(f\"  Difference:         {abs(regression_results['slope'] - regression_filtered['slope']):.6f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Too few benchmarks remaining after filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 High-Coverage Benchmarks Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS CHECK 3: High-Coverage Benchmarks Only (≥70%)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def filter_low_coverage_benchmarks(df_wide, min_coverage=0.7):\n",
    "    \"\"\"\n",
    "    Keep only benchmarks with at least min_coverage of non-missing data.\n",
    "    \"\"\"\n",
    "    benchmark_cols = [col for col in df_wide.columns if col not in ['model', 'eci']]\n",
    "    \n",
    "    keep_benchmarks = []\n",
    "    for col in benchmark_cols:\n",
    "        coverage = df_wide[col].notna().mean()\n",
    "        if coverage >= min_coverage:\n",
    "            keep_benchmarks.append(col)\n",
    "        else:\n",
    "            print(f\"  Excluding {col}: coverage = {coverage:.1%}\")\n",
    "    \n",
    "    print(f\"\\nKept {len(keep_benchmarks)}/{len(benchmark_cols)} benchmarks\")\n",
    "    \n",
    "    return df_wide[['model', 'eci'] + keep_benchmarks]\n",
    "\n",
    "# Filter benchmarks\n",
    "df_wide_high_cov = filter_low_coverage_benchmarks(df_wide, min_coverage=0.7)\n",
    "\n",
    "if len(df_wide_high_cov.columns) > 3:  # At least one benchmark remains\n",
    "    # Preprocess and analyze\n",
    "    X_high_cov, _, _, _ = preprocess_data(df_wide_high_cov, use_logit=True, k=5)\n",
    "    pca_results_high_cov = perform_pca_per_bin(X_high_cov, eci_values, n_bins=8)\n",
    "    regression_high_cov = analyze_slodr_hypothesis(pca_results_high_cov)\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  All benchmarks:      slope = {regression_results['slope']:.6f}\")\n",
    "    print(f\"  High-coverage only:  slope = {regression_high_cov['slope']:.6f}\")\n",
    "    print(f\"  Difference:          {abs(regression_results['slope'] - regression_high_cov['slope']):.6f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Too few benchmarks remaining after filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive markdown summary report.\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "# SLODR Analysis Summary Report\n",
    "\n",
    "## Dataset Overview\n",
    "- **Total models**: {len(df_wide)}\n",
    "- **Total benchmarks**: {len(benchmark_names)}\n",
    "- **ECI range**: [{eci_values.min():.2f}, {eci_values.max():.2f}]\n",
    "- **Mean benchmark coverage**: {df_wide[benchmark_names].notna().mean().mean():.1%}\n",
    "\n",
    "## Main Results\n",
    "\n",
    "### Linear Regression\n",
    "- **Slope**: {regression_results['slope']:.6f}\n",
    "- **Intercept**: {regression_results['intercept']:.6f}\n",
    "- **R²**: {regression_results['r2']:.4f}\n",
    "- **Pearson correlation**: {regression_results['correlation']:.4f} (p = {regression_results['correlation_p_value']:.4f})\n",
    "\n",
    "### Bootstrap Confidence Intervals (95%)\n",
    "- **Lower bound**: {bootstrap_results['lower_bound']:.6f}\n",
    "- **Upper bound**: {bootstrap_results['upper_bound']:.6f}\n",
    "- **Bootstrap mean**: {bootstrap_results['mean']:.6f}\n",
    "- **Bootstrap std**: {bootstrap_results['std']:.6f}\n",
    "\n",
    "### Permutation Test\n",
    "- **P-value**: {permutation_results['p_value']:.4f}\n",
    "- **Significance level**: {'✓ Significant at p < 0.05' if permutation_results['p_value'] < 0.05 else '✗ Not significant at p < 0.05'}\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "### SLODR Hypothesis\n",
    "The **Spearman's Law of Diminishing Returns** hypothesis predicts that as general capability (ECI) increases, \n",
    "the first principal component should explain **less variance** in benchmark performance, indicating greater \n",
    "differentiation of specific abilities.\n",
    "\n",
    "**Result**: {'✓ **SUPPORTED**' if regression_results['slope'] < 0 and permutation_results['p_value'] < 0.05 else '✗ **NOT SUPPORTED**'}\n",
    "\n",
    "- The observed slope is **{'negative' if regression_results['slope'] < 0 else 'positive'}** ({regression_results['slope']:.6f})\n",
    "- The relationship is **{'statistically significant' if permutation_results['p_value'] < 0.05 else 'not statistically significant'}** (p = {permutation_results['p_value']:.4f})\n",
    "- The 95% confidence interval {'**excludes zero**' if (bootstrap_results['lower_bound'] < 0 and bootstrap_results['upper_bound'] < 0) or (bootstrap_results['lower_bound'] > 0 and bootstrap_results['upper_bound'] > 0) else '**includes zero**'}\n",
    "\n",
    "## Robustness Checks\n",
    "\n",
    "### 1. Without Logit Transformation\n",
    "- **Slope**: {regression_no_logit['slope']:.6f}\n",
    "- **Difference from main**: {abs(regression_results['slope'] - regression_no_logit['slope']):.6f}\n",
    "\n",
    "### 2. Excluding Near-Ceiling Benchmarks\n",
    "\"\"\"\n",
    "    \n",
    "    if 'regression_filtered' in globals():\n",
    "        report += f\"\"\"\n",
    "- **Slope**: {regression_filtered['slope']:.6f}\n",
    "- **Difference from main**: {abs(regression_results['slope'] - regression_filtered['slope']):.6f}\n",
    "\"\"\"\n",
    "    else:\n",
    "        report += \"- Not applicable (too few benchmarks after filtering)\\n\"\n",
    "    \n",
    "    report += \"\\n### 3. High-Coverage Benchmarks Only (≥70%)\\n\"\n",
    "    \n",
    "    if 'regression_high_cov' in globals():\n",
    "        report += f\"\"\"\n",
    "- **Slope**: {regression_high_cov['slope']:.6f}\n",
    "- **Difference from main**: {abs(regression_results['slope'] - regression_high_cov['slope']):.6f}\n",
    "\"\"\"\n",
    "    else:\n",
    "        report += \"- Not applicable (too few benchmarks after filtering)\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "## What to Inspect if Results Look Wrong\n",
    "\n",
    "### Potential Issues to Check:\n",
    "\n",
    "1. **Ceiling Effects**\n",
    "   - Look at score distributions for each benchmark\n",
    "   - Check if top models cluster near perfect scores (>0.95)\n",
    "   - Solution: Exclude saturated benchmarks\n",
    "\n",
    "2. **Model Family Effects**\n",
    "   - Different model families may have systematic biases\n",
    "   - Check if results hold within single model families\n",
    "   - Solution: Perform stratified analysis by organization\n",
    "\n",
    "3. **Missing Data Patterns**\n",
    "   - Are higher-ECI models missing specific benchmarks?\n",
    "   - Check correlation between missingness and ECI\n",
    "   - Solution: Use only high-coverage benchmarks\n",
    "\n",
    "4. **Bin Size Effects**\n",
    "   - Try different numbers of bins (4, 6, 10, 12)\n",
    "   - Check if results are stable across bin counts\n",
    "   - Solution: Use sliding windows instead of discrete bins\n",
    "\n",
    "5. **Outlier Models**\n",
    "   - Identify models with unusual benchmark patterns\n",
    "   - Check for evaluation artifacts or data errors\n",
    "   - Solution: Perform sensitivity analysis excluding outliers\n",
    "\n",
    "6. **Benchmark Diversity**\n",
    "   - Are all benchmarks measuring similar skills?\n",
    "   - Check pairwise correlations between benchmarks\n",
    "   - Solution: Select diverse, low-correlation benchmarks\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "### Data Files\n",
    "- `output/wide_matrix.csv` - Complete data matrix (models × benchmarks)\n",
    "- `output/pca_results.csv` - PCA metrics per ECI bin\n",
    "- `output/summary_statistics.csv` - All statistical results\n",
    "\n",
    "### Visualizations\n",
    "- `output/evr1_vs_eci.png` - Main SLODR plot with regression line\n",
    "- `output/permutation_test.png` - Permutation test distribution\n",
    "- `output/bootstrap_distribution.png` - Bootstrap confidence intervals\n",
    "- `output/bin_analysis.png` - EVR1 and sample sizes by bin\n",
    "\n",
    "## Recommendations for Further Analysis\n",
    "\n",
    "1. **Sliding Window Analysis**: Use overlapping windows instead of discrete bins for smoother trends\n",
    "2. **Benchmark Clustering**: Group similar benchmarks and analyze separately\n",
    "3. **Temporal Analysis**: Check if SLODR effect changes with model release date\n",
    "4. **Organization-Specific Analysis**: Test hypothesis within individual AI labs\n",
    "5. **Non-Linear Models**: Try polynomial or spline regression for non-linear trends\n",
    "\n",
    "---\n",
    "\n",
    "*Analysis completed: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "markdown_report = generate_markdown_report()\n",
    "\n",
    "# Save to file\n",
    "with open(output_dir / 'SLODR_REPORT.md', 'w') as f:\n",
    "    f.write(markdown_report)\n",
    "\n",
    "print(\"✓ Saved: output/SLODR_REPORT.md\\n\")\n",
    "\n",
    "# Display report\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(markdown_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "All results have been saved to the `output/` directory. Check the `SLODR_REPORT.md` file for a comprehensive summary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
